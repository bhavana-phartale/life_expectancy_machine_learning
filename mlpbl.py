# -*- coding: utf-8 -*-
"""MLPBL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oA0iJB7u7dZd5MDSP-44ECZOmLwHObnB
"""

import pandas as pd
df=pd.read_csv(r'Life Expectancy Data.csv')
df.head()

drop_columns = ['Year']
df = df.drop(columns=drop_columns)

"""descriptive statistics"""

df.describe()

"""DATA preprocessing

"""

df.isnull().sum()

import pandas as pd
from sklearn.impute import SimpleImputer

# Impute missing values for numerical columns with the mean
imputer = SimpleImputer(strategy='median')

# Get the correct column names from the DataFrame
numerical_cols = ['Alcohol', 'Hepatitis B', ' BMI ', 'Total expenditure', 'GDP', 'Population', 'Schooling']
# Corrected ' BMI ' to 'BMI'

# Make sure the columns exist in the DataFrame. If not, print a warning.
for col in numerical_cols:
    if col not in df.columns:
        print(f"Warning: Column '{col}' not found in DataFrame. Skipping imputation for this column.")

# Filter the columns that actually exist in the DataFrame.
existing_cols = [col for col in numerical_cols if col in df.columns]

# Apply imputation only on the existing columns.
if existing_cols:  # Check if there are any existing columns to impute
    df[existing_cols] = imputer.fit_transform(df[existing_cols])
else:
    print("Warning: None of the specified columns were found in the DataFrame. No imputation performed.")

df.columns

from sklearn.impute import SimpleImputer

# Impute missing values for numerical columns with the median
imputer = SimpleImputer(strategy='median')

#Corrected the column name to remove extra spaces and align with the DataFrame columns
numerical_cols = ['Alcohol', 'Hepatitis B', 'BMI', 'Total expenditure', 'GDP', 'Population', 'Schooling']

# Verify if the columns are present in the dataframe
for col in numerical_cols:
    if col not in df.columns:
        print(f"Warning: Column '{col}' not found in DataFrame. Skipping imputation for this column.")
        numerical_cols.remove(col) # Removing columns not found in the DataFrame

# Generate histograms after ensuring the correct column names and removing non-existent columns
df[numerical_cols].hist(figsize=(12, 8))

# Perform imputation on the correctly named existing columns
df[numerical_cols] = imputer.fit_transform(df[numerical_cols])

df.isnull().sum()

df.columns = df.columns.str.strip()
df.columns

# Original code causing error
# df[['thinness 1-19 years', 'thinness 5-9 years']].hist(figsize=(8, 4))

# Strip leading/trailing spaces from column names
df.columns = df.columns.str.strip()

# Access columns with the correct names
df[['thinness  1-19 years', 'thinness 5-9 years','Income composition of resources']].hist(figsize=(8, 4))

imputer_median = SimpleImputer(strategy='median')
# Access columns with the correct names, which may include extra spaces
# For example, if the column name is 'thinness  1-19 years', use that instead of 'thinness 1-19 years'
df[['thinness  1-19 years', 'thinness 5-9 years','Income composition of resources']] = imputer_median.fit_transform(df[['thinness  1-19 years', 'thinness 5-9 years','Income composition of resources']])

df.dropna(subset=['Life expectancy',"Adult Mortality","Polio"], inplace=True)

df.isnull().sum()

"""feature selection

"""



import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Select only numerical features for correlation analysis
numerical_df = df.select_dtypes(include=['number'])

# Calculate the correlation matrix using the numerical DataFrame
correlation_matrix = numerical_df.corr()

# Display the correlation coefficients for life expectancy
print(correlation_matrix['Life expectancy'].sort_values(ascending=False))

# Plot the heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Heatmap')
plt.show()

import pandas as pd

# Select only numerical features for correlation analysis
numerical_df = df.select_dtypes(include=['number'])

# Calculate the correlation matrix using the numerical DataFrame
correlation = numerical_df.corr()
print(correlation['Life expectancy'].sort_values(ascending=False))

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt


# Set the style of seaborn
sns.set(style="whitegrid")

# Create a box plot for each numeric column in the dataset
plt.figure(figsize=(15, 10))
for i, column in enumerate(df.select_dtypes(include=['float64', 'int64']).columns):
    plt.subplot(len(df.select_dtypes(include=['float64', 'int64']).columns)//3 + 1, 3, i + 1)
    sns.boxplot(x=df[column])
    plt.title(f'Box Plot of {column}')
    plt.xlabel(column)

plt.tight_layout()
plt.show()

import numpy as np
import pandas as pd

# Display the actual column names in the DataFrame
print("Column names in DataFrame:", df.columns)

# Optionally, inspect the first few rows to understand the structure
print(df.head())

# Strip any leading or trailing spaces from column names
df.columns = df.columns.str.strip()

# List of columns with potential outliers
outlier_cols = [
    'Adult_Mortality', 'infant_deaths', 'Alcohol', 'percentage_expenditure', 'Hepatitis_B',
    'Measles', 'BMI', 'under-five_deaths', 'Polio', 'Total_expenditure',
    'Diphtheria', 'HIV/AIDS', 'GDP', 'Population',
    'thinness_1-19_years', 'thinness_5-9_years',
    'Income_composition_of_resources', 'Schooling'
]

# Filter the outlier columns to only include those that are present in the DataFrame
outlier_cols = [col for col in outlier_cols if col in df.columns]

# Perform outlier handling for each specified column
for col_name in outlier_cols:
    # Calculate quartiles and IQR
    q1 = df[col_name].quantile(0.25)
    q3 = df[col_name].quantile(0.75)
    iqr = q3 - q1

    # Define the lower and upper bounds for outliers
    lower_bound = q1 - 1.5 * iqr
    upper_bound = q3 + 1.5 * iqr

    # Replace outliers with the median value of the column
    df[col_name] = np.where((df[col_name] > upper_bound) | (df[col_name] < lower_bound),
                            np.median(df[col_name]), df[col_name])

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
import numpy as np

# Define the features you want to check
features = ['Schooling', 'Income composition of resources', 'BMI',
            'Diphtheria', 'Polio', 'GDP', 'Alcohol',
            'percentage expenditure', 'Adult Mortality',
            'HIV/AIDS', 'thinness  1-19 years',
            'thinness 5-9 years']

# Create scatter plots with regression line
for feature in features:
    plt.figure(figsize=(8, 5))

    # Scatter plot
    sns.scatterplot(x=df[feature], y=df['Life expectancy'], color='blue')

    # Fit a linear regression line
    X = df[[feature]]
    y = df['Life expectancy']
    model = LinearRegression()
    model.fit(X, y)
    y_pred = model.predict(X)

    # Plot the regression line
    plt.plot(df[feature], y_pred, color='red')

    plt.title(f'Scatter plot of {feature} vs Life Expectancy')
    plt.xlabel(feature)
    plt.ylabel('Life Expectancy')
    plt.grid()
    plt.show()

# Select only numerical columns
numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns
fig, axes = plt.subplots(4, 5, figsize=(20, 16))
fig.suptitle('Boxplots of Numerical Columns', fontsize=16)

# Flatten the axes array for easy iteration
axes = axes.flatten()

# Plot boxplots for each numerical column
for i, col in enumerate(numerical_cols):
    sns.boxplot(y=df[col], ax=axes[i])
    axes[i].set_title(col)

# Remove any empty subplots
for j in range(len(numerical_cols), len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

# Importing necessary libraries
import pandas as pd
from sklearn.preprocessing import StandardScaler

# Assuming df is your DataFrame with the selected features

# Create a new DataFrame with selected features
selected_features = df[['Schooling', 'Income composition of resources', 'BMI',
                        'Diphtheria', 'Polio', 'GDP', 'Alcohol',
                        'percentage expenditure', 'Adult Mortality',
                        'HIV/AIDS', 'thinness  1-19 years',
                        'thinness 5-9 years', 'Life expectancy']] # Added 'Life expectancy' here

# Separate features and target
X = selected_features.drop('Life expectancy', axis=1)  # Features
y = selected_features['Life expectancy']  # Target variable

# Initialize the StandardScaler
scaler = StandardScaler()

# Fit and transform the feature data
scaled_data = scaler.fit_transform(X)

# Convert back to DataFrame for easier handling
scaled_df = pd.DataFrame(scaled_data, columns=X.columns)


# Display the scaled DataFrame
print(scaled_df.head())

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.linear_model import LinearRegression
import pandas as pd
model = LinearRegression()
# Perform cross-validation with 5 folds
cv_scores = cross_val_score(model, X, y, cv=5)  # 5-fold cross-validation

# Print the cross-validation scores
print(f'Cross-validation scores: {cv_scores}')
print(f'Mean CV score: {cv_scores.mean()}')

from sklearn.linear_model import LinearRegression
from sklearn.model_selection import cross_val_predict
from sklearn.metrics import mean_squared_error
import numpy as np

# Create a linear regression model
model = LinearRegression()

# Use cross_val_predict to get predictions
predicted = cross_val_predict(model, X, y, cv=5)

# Calculate Mean Squared Error
mse = mean_squared_error(y, predicted)

# Print the Mean Squared Error
print(f'Mean Squared Error: {mse:.4f}')



# Import necessary libraries
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error

# Initialize the Random Forest Regressor
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)

# Evaluate the model using cross-validation
cv_scores = cross_val_score(rf_model, X, y, cv=5)  # 5-fold cross-validation

# Fit the model on the entire dataset
rf_model.fit(X, y)

# Make predictions
predictions = rf_model.predict(X)

# Calculate Mean Squared Error
mse = mean_squared_error(y, predictions)

# Print results
print("Cross-validation scores:", cv_scores)
print("Mean CV score:", cv_scores.mean())
print("Mean Squared Error:", mse)

#model evaluation
from sklearn.metrics import mean_absolute_error, r2_score

# Predictions
predictions = rf_model.predict(X)

# Calculate additional metrics
mae = mean_squared_error(y, predictions)
r2 = r2_score(y, predictions)

print("Mean Absolute Error:", mae)
print("R² Score:", r2)

# Plot predictions vs actual
plt.scatter(y, predictions)
plt.xlabel('Actual Life Expectancy')
plt.ylabel('Predicted Life Expectancy')
plt.title('Actual vs Predicted')
plt.plot([y.min(), y.max()], [y.min(), y.max()], '--r')  # Diagonal line
plt.show()

#svm
from sklearn.svm import SVR
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
# Initialize the Support Vector Regressor
svr_model = SVR()

# Perform cross-validation
cv_scores = cross_val_score(svr_model, X, y, cv=5)

# Fit the model on the entire dataset
svr_model.fit(X, y)

# Make predictions
y_pred = svr_model.predict(X)

# Calculate Mean Squared Error
mse = mean_squared_error(y, y_pred)

# Display the results
print("Cross-validation scores:", cv_scores)
print("Mean CV score:", cv_scores.mean())
print("Mean Squared Error:", mse)

from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
import numpy as np

from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_predict
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

# Initialize the Decision Tree Regressor
decision_tree = DecisionTreeRegressor()

# Perform 5-fold cross-validation and get predictions
predictions = cross_val_predict(decision_tree, X, y, cv=5)

# Calculate the Mean Squared Error
mse = mean_squared_error(y, predictions)

# Calculate the R² score
r2 = r2_score(y, predictions)

print(f"Mean Squared Error: {mse}")
print(f"R² Score: {r2}")

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Load your dataset
# df = pd.read_csv('path_to_your_dataset.csv')

# Select features and target variable
X = df[['Schooling', 'Income composition of resources', 'BMI',
         'Diphtheria', 'Polio', 'GDP', 'Alcohol',
         'percentage expenditure', 'Adult Mortality',
         'HIV/AIDS', 'thinness  1-19 years',
         'thinness 5-9 years']]
y = df['Life expectancy']

# Split the dataset into training (70%) and testing (30%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Random Forest
random_forest_regressor = RandomForestRegressor(random_state=42)
random_forest_regressor.fit(X_train, y_train)
y_pred_rf = random_forest_regressor.predict(X_test)

mse_rf = mean_squared_error(y_test, y_pred_rf)
r2_rf = r2_score(y_test, y_pred_rf)
print(f"Random Forest - Mean Squared Error: {mse_rf}, R² Score: {r2_rf}")

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Load your dataset
# df = pd.read_csv('path_to_your_dataset.csv')

# Select features and target variable
X = df[['Schooling', 'Income composition of resources', 'BMI',
         'Diphtheria', 'Polio', 'GDP', 'Alcohol',
         'percentage expenditure', 'Adult Mortality',
         'HIV/AIDS', 'thinness  1-19 years',
         'thinness 5-9 years']]
y = df['Life expectancy']

# Split the dataset into training (70%) and testing (30%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Decision Tree
decision_tree_regressor = DecisionTreeRegressor(random_state=42)
decision_tree_regressor.fit(X_train, y_train)
y_pred_dt = decision_tree_regressor.predict(X_test)

mse_dt = mean_squared_error(y_test, y_pred_dt)
r2_dt = r2_score(y_test, y_pred_dt)
print(f"Decision Tree - Mean Squared Error: {mse_dt}, R² Score: {r2_dt}")

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Load your dataset
# df = pd.read_csv('path_to_your_dataset.csv')  # Example for loading dataset
# X = df.drop('target_column', axis=1)  # Replace 'target_column' with your actual target column
# y = df['target_column']  # Replace with your target variable

# Split the dataset into training and testing sets (70% train, 30% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Initialize models
linear_model = LinearRegression()
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
svr_model = SVR()
decision_tree = DecisionTreeRegressor()

# Train and evaluate Linear Regression
linear_model.fit(X_train, y_train)
linear_predictions = linear_model.predict(X_test)
linear_mse = mean_squared_error(y_test, linear_predictions)
linear_r2 = r2_score(y_test, linear_predictions)
print(f"Linear Regression Mean Squared Error: {linear_mse:.4f}")
print(f"Linear Regression R² Score: {linear_r2:.4f}")

# Train and evaluate Random Forest Regression
rf_model.fit(X_train, y_train)
rf_predictions = rf_model.predict(X_test)
rf_mse = mean_squared_error(y_test, rf_predictions)
rf_r2 = r2_score(y_test, rf_predictions)
print(f"Random Forest Regression Mean Squared Error: {rf_mse:.4f}")
print(f"Random Forest Regression R² Score: {rf_r2:.4f}")

# Train and evaluate Support Vector Regression
svr_model.fit(X_train, y_train)
svr_predictions = svr_model.predict(X_test)
svr_mse = mean_squared_error(y_test, svr_predictions)
svr_r2 = r2_score(y_test, svr_predictions)
print(f"Support Vector Regression Mean Squared Error: {svr_mse:.4f}")
print(f"Support Vector Regression R² Score: {svr_r2:.4f}")

# Train and evaluate Decision Tree Regression
decision_tree.fit(X_train, y_train)
dt_predictions = decision_tree.predict(X_test)
dt_mse = mean_squared_error(y_test, dt_predictions)
dt_r2 = r2_score(y_test, dt_predictions)
print(f"Decision Tree Regression Mean Squared Error: {dt_mse:.4f}")
print(f"Decision Tree Regression R² Score: {dt_r2:.4f}")

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor
import matplotlib.pyplot as plt
import seaborn as sns

# Initialize and fit the Random Forest model
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X, y)

# Get feature importances
importances = rf_model.feature_importances_
feature_importances = pd.Series(importances, index=X.columns)

# Sort feature importances
feature_importances_sorted = feature_importances.sort_values(ascending=False)

# Print the feature importances
print(feature_importances_sorted)
# Show all feature importances
plt.figure(figsize=(10, 6))
feature_importances.plot(kind='barh', color='skyblue')
plt.title('Feature Importances from Random Forest')
plt.xlabel('Importance Score')
plt.ylabel('Features')
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# List of columns you want to visualize against 'Life expectancy'
columns_to_plot = ['Schooling', 'Income composition of resources', 'BMI',
         'Diphtheria', 'Polio', 'GDP', 'Alcohol',
         'percentage expenditure', 'Adult Mortality',
         'HIV/AIDS', 'thinness  1-19 years',
         'thinness 5-9 years']

# Iterate over the columns and create scatter plots
for col in columns_to_plot:
    plt.figure(figsize=(6, 4))
    sns.scatterplot(x=df[col], y=df['Life expectancy'])

    # Calculate and plot the mean line
    mean_life_expectancy = df['Life expectancy'].mean()
    plt.axhline(mean_life_expectancy, color='red', linestyle='--', label=f"Mean Life Expectancy: {mean_life_expectancy:.2f}")

    plt.title(f'Scatter plot of {col} vs Life expectancy')
    plt.xlabel(col)
    plt.ylabel('Life expectancy')
    plt.legend() # Show the legend to display the mean line label
    plt.show()

# Calculate the correlation matrix
numerical_df = df.select_dtypes(include=np.number)  # Select only numerical columns
correlation_matrix = numerical_df.corr()

# Print correlation with target variable
print(correlation_matrix['Life expectancy'].sort_values(ascending=False)) # Assuming 'Life expectancy' is the target

# Heatmap to visualize the correlation matrix
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

import pandas as pd
from sklearn.model_selection import KFold, cross_val_score
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler

# Load and preprocess the dataset
# Assuming you have your data in a DataFrame 'df'

# Step 1: Split the data into features (X) and target (y)
X = df[['Schooling', 'Income composition of resources', 'BMI',
         'Diphtheria', 'Polio', 'GDP', 'Alcohol',
         'percentage expenditure', 'Adult Mortality',
         'HIV/AIDS', 'thinness  1-19 years',
         'thinness 5-9 years']]
y = df['Life expectancy']

# Step 4: Define the Linear Regression model
model = LinearRegression()

# Step 5: Define K-Fold cross-validation (with 5 folds)
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# Step 6: Perform cross-validation using R² score as the evaluation metric
cv_results = cross_val_score(model, X, y, cv=kf, scoring='r2')

# Step 7: Print the R² score for each fold and the average R² score
print(f'R² score for each fold: {cv_results}')
print(f'Average R² score: {cv_results.mean()}')

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_squared_error
# Step 1: Split the data into features (X) and target (y)
X = df[['Schooling', 'Income composition of resources', 'BMI',
         'Diphtheria', 'Polio', 'GDP', 'Alcohol',
         'percentage expenditure', 'Adult Mortality',
         'HIV/AIDS', 'thinness  1-19 years',
         'thinness 5-9 years']]
y = df['Life expectancy']
# Step 4: Split the data into training and testing sets (80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 5: Define the Random Forest model
model = RandomForestRegressor(n_estimators=100, random_state=42)

# Step 6: Train the model using the training set
model.fit(X_train, y_train)

# Step 7: Make predictions on the test set
y_pred = model.predict(X_test)

# Step 8: Evaluate the model's performance using R² score and Mean Squared Error (MSE)
r2 = r2_score(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)

# Step 9: Print the evaluation metrics
print(f'R² score: {r2}')
print(f'Mean Squared Error: {mse}')

import pandas as pd
from sklearn.model_selection import StratifiedKFold, cross_val_score
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, make_scorer
from sklearn.preprocessing import StandardScaler

X = df[['Schooling', 'Income composition of resources', 'BMI',
         'Diphtheria', 'Polio', 'GDP', 'Alcohol',
         'percentage expenditure', 'Adult Mortality',
         'HIV/AIDS', 'thinness  1-19 years',
         'thinness 5-9 years']]
y = df['Life expectancy']
# Step 4: Define the Linear Regression model
model = LinearRegression()
# Step 5: Define Stratified K-Fold (with 5 splits)
# Since StratifiedKFold is typically used for classification,
# we'll use 'y' and create bins for stratification purposes.
y_binned = pd.cut(y, bins=5, labels=False)  # Stratifying based on binning the target variable

strat_kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Step 6: Perform cross-validation using R² score as the evaluation metric
r2_scorer = make_scorer(r2_score)
cv_results = cross_val_score(model, X, y_binned, cv=strat_kf, scoring=r2_scorer)

# Step 7: Print the R² score for each fold and the average R² score
print(f'R² score for each fold: {cv_results}')
print(f'Average R² score: {cv_results.mean()}')

import pandas as pd
from sklearn.model_selection import LeaveOneOut, cross_val_score
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, make_scorer
from sklearn.preprocessing import StandardScaler
# Step 1: Select a subset of the dataset (e.g., first 10 rows)
subset_df = df.head(10)  # Change the number as needed

X_subset = df[['Schooling', 'Income composition of resources', 'BMI',
         'Diphtheria', 'Polio', 'GDP', 'Alcohol',
         'percentage expenditure', 'Adult Mortality',
         'HIV/AIDS', 'thinness  1-19 years',
         'thinness 5-9 years']]
y_subset = df['Life expectancy']
scaler = StandardScaler()
X_scaled_subset = scaler.fit_transform(X_subset)
# Step 5: Define the Linear Regression model
model = LinearRegression()

# Step 6: Define Leave-One-Out Cross-Validation
loo = LeaveOneOut()

# Step 7: Perform Leave-One-Out cross-validation and compute R² scores
cv_results_loo = cross_val_score(model, X_scaled_subset, y_subset, cv=loo, scoring='r2')

# Step 8: Print the R² score for each iteration and the average R² score
print(f'Leave-One-Out R² score for each fold: {cv_results_loo}')
print(f'Average Leave-One-Out R² score: {cv_results_loo.mean()}')

# Step 8: Print the R² score for each iteration and the average R² score
print(f'Leave-One-Out R² score for each fold: {cv_results_loo}')
print(f'Average Leave-One-Out R² score: {cv_results_loo.mean()}')

import pandas as pd
from sklearn.ensemble import BaggingRegressor
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

X = df[['Schooling', 'Income composition of resources', 'BMI',
         'Diphtheria', 'Polio', 'GDP', 'Alcohol',
         'percentage expenditure', 'Adult Mortality',
         'HIV/AIDS', 'thinness  1-19 years',
         'thinness 5-9 years']]
y = df['Life expectancy']
# Step 2: Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 3: Define the base estimator (Linear Regression) and Bagging Regressor
base_model = LinearRegression()
# Use 'estimator' instead of 'base_estimator' for older scikit-learn versions
bagging_model = BaggingRegressor(estimator=base_model, n_estimators=10, random_state=42)

# Step 4: Train the Bagging model
bagging_model.fit(X_train, y_train)

# Step 5: Make predictions
y_pred = bagging_model.predict(X_test)

# Step 6: Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f'Mean Squared Error: {mse}')
print(f'R² Score: {r2}')

import pandas as pd
from sklearn.ensemble import AdaBoostRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
X = df[['Schooling', 'Income composition of resources', 'BMI',
         'Diphtheria', 'Polio', 'GDP', 'Alcohol',
         'percentage expenditure', 'Adult Mortality',
         'HIV/AIDS', 'thinness  1-19 years',
         'thinness 5-9 years']]
y = df['Life expectancy']snipp
# Step 3: Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 4: Define the base estimator (Decision Tree) and AdaBoost Regressor
base_model = DecisionTreeRegressor(max_depth=1)  # Using decision stumps as weak learners
# Use 'estimator' instead of 'base_estimator' for older scikit-learn versions
ada_boost_model = AdaBoostRegressor(estimator=base_model, n_estimators=50, random_state=42)

# Step 5: Train the AdaBoost model
ada_boost_model.fit(X_train, y_train)
# Step 6: Make predictions
y_pred = ada_boost_model.predict(X_test)

# Step 7: Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print(f'Mean Squared Error: {mse}')
print(f'R² Score: {r2}')

import pandas as pd
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

X = df[['Schooling', 'Income composition of resources', 'BMI',
         'Diphtheria', 'Polio', 'GDP', 'Alcohol',
         'percentage expenditure', 'Adult Mortality',
         'HIV/AIDS', 'thinness  1-19 years',
         'thinness 5-9 years']]
y = df['Life expectancy']
# Step 3: Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 4: Define the Gradient Boosting model
gbm_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)

# Step 5: Train the GBM model
gbm_model.fit(X_train, y_train)
# Step 6: Make predictions
y_pred = gbm_model.predict(X_test)

# Step 7: Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print(f'Mean Squared Error: {mse}')
print(f'R² Score: {r2}')

import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score


X = df[['Schooling', 'Income composition of resources', 'BMI',
         'Diphtheria', 'Polio', 'GDP', 'Alcohol',
         'percentage expenditure', 'Adult Mortality',
         'HIV/AIDS', 'thinness  1-19 years',
         'thinness 5-9 years']]
y = df['Life expectancy']

# Step 3: Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 4: Define the XGBoost model
xgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, learning_rate=0.1, max_depth=3)

# Step 5: Train the model
xgb_model.fit(X_train, y_train)

# Step 6: Make predictions
y_pred = xgb_model.predict(X_test)

# Step 7: Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f'Mean Squared Error: {mse}')
print(f'R² Score: {r2}')

